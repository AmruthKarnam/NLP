{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import *\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.layers import LSTM, Dropout, Activation , Bidirectional,Dense,Flatten,Activation,RepeatVector,Permute,Multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import*\n",
    "from keras import initializers, regularizers, constraints, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "import codecs\n",
    "import csv\n",
    "from nltk import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import gensim\n",
    "import time\n",
    "import os\n",
    "import sys \n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadOpen(filename,Labelfile):\n",
    "    data = []\n",
    "    with codecs.open(filename, 'r',encoding=\"utf-8\", errors=\"replace\") as readFile:\n",
    "        reader = csv.reader(readFile)\n",
    "        lines = list(reader)\n",
    "    count = 0\n",
    "    for i in lines:\n",
    "        temp = []\n",
    "        sentence = ' '.join(i)\n",
    "        for j in word_tokenize(sentence):\n",
    "            temp.append(j.lower()) \n",
    "            count += 1\t  \n",
    "        data.append(temp)\n",
    "    labels_pd = pd.read_csv(Labelfile,index_col=False)\n",
    "    labels = numpy.asarray(labels_pd)\n",
    "\n",
    "    return data[1:],labels, count-1\n",
    "\n",
    "\n",
    "def extract_emojis(sentence):\n",
    "    return [word for word in sentence.split() if str(word.encode('unicode-escape'))[2] == '\\\\' ]\n",
    "\n",
    "\n",
    "def char_is_emoji(character):\n",
    "    if character in emoji.UNICODE_EMOJI:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(docs,count):\n",
    "    # prepare tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(docs)\n",
    "    # vocab_size = len(t.word_index) + 1\n",
    "    # print(vocab_size)\n",
    "    # integer encode the documents\n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    # pad documents to a max length of 4 words\n",
    "    # max_length = 4\n",
    "    padded_docs = pad_sequences(encoded_docs, padding='post')\n",
    "    l = len(padded_docs[0])\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open('glove.6B.300d.txt',encoding=\"utf8\")\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    e2v = gensim.models.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "    nf = 0\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((count, 300))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # print(word)\n",
    "            new_em = []\n",
    "            em = extract_emojis(word)\n",
    "            for ej in em:\n",
    "                for c in ej:\n",
    "                    if char_is_emoji(c):\n",
    "                        new_em.append(c)\n",
    "            # print(new_em)\n",
    "            try:\n",
    "                if new_em:\n",
    "                        row = []\n",
    "                        for e in new_em:\n",
    "\n",
    "                            row.append(e2v[e])\n",
    "                        embedding_matrix[i] = np.average(np.asarray(row),axis=0).tolist()\n",
    "                else:\n",
    "                    embedding_matrix[i] = [0] * 300\n",
    "            except:\n",
    "                embedding_matrix[i] = [0] * 300\n",
    "                nf += 1\n",
    "\n",
    "    print(str(nf)+\" words not found in vocabulary\")\n",
    "\n",
    "    return padded_docs, embedding_matrix,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepModel(count,embedding_matrix,l,lrate=0.01):\n",
    "    #model1 = Sequential()\n",
    "    #model2 = Sequential()\n",
    "\n",
    "    input = Input(shape = (926,))\n",
    "\n",
    "    embedded1 = Embedding(count,300, weights=[embedding_matrix],trainable=False)(input)\n",
    "    embedded2 = Embedding(count,300, weights=[embedding_matrix],trainable=False)(input)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(100,kernel_initializer='he_normal', activation='sigmoid', dropout=0.5,recurrent_dropout=0.5, unroll=False, return_sequences=False))(embedded1)\n",
    "    lstm2 = Bidirectional(LSTM(100,kernel_initializer='he_normal', activation='sigmoid', dropout=0.5,recurrent_dropout=0.5, unroll=False, return_sequences=True))(embedded2)\n",
    "\n",
    "    print(\"here2\\n\")\n",
    "\n",
    "    dense = Dense(1, activation='tanh')(lstm2)\n",
    "    flatten = Flatten()(dense)\n",
    "    activation_1 = Activation('softmax')(flatten)\n",
    "    repeat_vector = RepeatVector(100)(activation_1)\n",
    "    permute = Permute([2,1])(repeat_vector)\n",
    "    dense_1 = Dense( 100, activation='sigmoid', name='attention_probs')(lstm1)\n",
    "\n",
    "    model1 = Model(inputs = input,output = dense_1)\n",
    "    model2 = Model(inputs = input,output = permute)\n",
    "\n",
    "    mergedOut = Multiply()([model1.output,model2.output])\n",
    "    mergedOut = Flatten()(mergedOut)\n",
    "    mergedOut = Dense(1,activation=\"sigmoid\")(mergedOut)\n",
    "    model = Model(inputs = input,output = mergedOut)\n",
    "    model.compile(optimizer=Adam(lr=lrate), loss='binary_crossentropy', metrics=['acc',f1,recall,precision])\n",
    "\n",
    "    print('No of parameter:', model.count_params())\n",
    "    print(model.summary())\n",
    "    print(K.eval(model.optimizer.lr))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [01:22, 4871.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "235 words not found in vocabulary\n",
      "Preparing model...\n",
      "here2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajeya/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"at...)`\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"pe...)`\n",
      "/home/ajeya/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ajeya/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "No of parameter: 59472302\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 926)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 926, 300)     29358900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 926, 200)     320800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 926, 1)       201         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 926)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 926, 300)     29358900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 926)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 100, 926)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_probs (Dense)         (None, 100)          20100       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 926, 100)     0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 926, 100)     0           attention_probs[0][0]            \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 92600)        0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            92601       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 59,472,302\n",
      "Trainable params: 754,502\n",
      "Non-trainable params: 58,717,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "0.01\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajeya/.local/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ajeya/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3500 samples, validate on 876 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "def ReadTest(filename,Labelfile):\n",
    "    data = []\n",
    "\n",
    "    with codecs.open(filename, 'r',encoding=\"utf-8\", errors=\"replace\") as readFile:\n",
    "        reader = csv.reader(readFile)\n",
    "        lines = list(reader)\n",
    "    count = 0\n",
    "    for i in lines:\n",
    "        temp = []\n",
    "        sentence = ' '.join(i)\n",
    "        for j in word_tokenize(sentence):\n",
    "            temp.append(j.lower()) \n",
    "            count += 1\n",
    "\n",
    "        data.append(temp)\n",
    "\n",
    "    labels_pd = pd.read_csv(Labelfile,index_col=False)\n",
    "    labels = numpy.array(labels_pd['Labels'])\n",
    "    # labels = numpy.asarray(labels_pd)\n",
    "\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(data)\n",
    "    encoded_docs = t.texts_to_sequences(data)\n",
    "    padded_docs = pad_sequences(encoded_docs, padding='post',maxlen=926)\n",
    "\n",
    "    return padded_docs,labels\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open('settings.json') as data_file:\n",
    "        data = json.load(data_file)\n",
    "\n",
    "    lrate = data[\"Model_settings\"][\"Learning_rate\"]\n",
    "    num_epochs = data[\"Model_settings\"][\"Epochs\"]\n",
    "\n",
    "    filename = data[\"FileNames\"][\"Training_file\"]\n",
    "    Labelfile = data[\"FileNames\"][\"Label_file\"]\n",
    "    # data,labels = ReadFile(filename,Labelfile)\n",
    "\n",
    "\n",
    "    print('Reading data...')\n",
    "    data,labels,count = ReadOpen(filename,Labelfile)\n",
    "    print('Getting Embeddings...')\n",
    "    padded_docs, embedding_matrix,l = Preprocess(data,count)\n",
    "    print('Preparing model...')\n",
    "    model = PrepModel(count,embedding_matrix,l,lrate)\n",
    "    print('Training...')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n",
    "    # X_test1, y_test1 = ReadTest(\"Testing_data.csv\",\"Testing_labels.csv\")\n",
    "    # print(\"length of X_test,y_test\")\n",
    "    # print(len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=1, mode='auto')\n",
    "    model.fit(X_train, y_train, validation_data=(X_val,y_val), nb_epoch=num_epochs, verbose=1, callbacks=[earlyStopping])\n",
    "    loss, accuracy,f1_score,recall_score,precision_score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    \n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    print('F1 Score: %f' % (f1_score*100))\n",
    "    print('Recall: %f' % (recall_score*100))\n",
    "    print('Precision: %f' % (precision_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
