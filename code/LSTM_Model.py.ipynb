{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0427 19:26:45.376163 20360 transport.py:43] unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import *\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.layers import LSTM, Dropout, Activation , Bidirectional,Dense,Flatten,Activation,RepeatVector,Permute,Multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import*\n",
    "from keras import initializers, regularizers, constraints, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "import codecs\n",
    "import csv\n",
    "from nltk import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import gensim\n",
    "import time\n",
    "import os\n",
    "import sys \n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadOpen(filename,Labelfile):\n",
    "    data = []\n",
    "    with codecs.open(filename, 'r',encoding=\"utf-8\", errors=\"replace\") as readFile:\n",
    "        reader = csv.reader(readFile)\n",
    "        lines = list(reader)\n",
    "    count = 0\n",
    "    for i in lines:\n",
    "        temp = []\n",
    "        sentence = ' '.join(i)\n",
    "        for j in word_tokenize(sentence):\n",
    "            temp.append(j.lower()) \n",
    "            count += 1\t  \n",
    "        data.append(temp)\n",
    "    labels_pd = pd.read_csv(Labelfile,index_col=False)\n",
    "    labels = numpy.asarray(labels_pd)\n",
    "\n",
    "    return data[1:],labels, count-1\n",
    "\n",
    "\n",
    "def extract_emojis(sentence):\n",
    "    return [word for word in sentence.split() if str(word.encode('unicode-escape'))[2] == '\\\\' ]\n",
    "\n",
    "\n",
    "def char_is_emoji(character):\n",
    "    if character in emoji.UNICODE_EMOJI:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(docs,count):\n",
    "    # prepare tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(docs)\n",
    "    # vocab_size = len(t.word_index) + 1\n",
    "    # print(vocab_size)\n",
    "    # integer encode the documents\n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    # pad documents to a max length of 4 words\n",
    "    # max_length = 4\n",
    "    padded_docs = pad_sequences(encoded_docs, padding='post')\n",
    "    l = len(padded_docs[0])\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open('glove.6B.300d.txt',encoding=\"utf8\")\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    e2v = gensim.models.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "    nf = 0\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((count, 300))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # print(word)\n",
    "            new_em = []\n",
    "            em = extract_emojis(word)\n",
    "            for ej in em:\n",
    "                for c in ej:\n",
    "                    if char_is_emoji(c):\n",
    "                        new_em.append(c)\n",
    "            # print(new_em)\n",
    "            try:\n",
    "                if new_em:\n",
    "                        row = []\n",
    "                        for e in new_em:\n",
    "\n",
    "                            row.append(e2v[e])\n",
    "                        embedding_matrix[i] = np.average(np.asarray(row),axis=0).tolist()\n",
    "                else:\n",
    "                    embedding_matrix[i] = [0] * 300\n",
    "            except:\n",
    "                embedding_matrix[i] = [0] * 300\n",
    "                nf += 1\n",
    "\n",
    "    print(str(nf)+\" words not found in vocabulary\")\n",
    "\n",
    "    return padded_docs, embedding_matrix,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepModel(count,embedding_matrix,l,lrate=0.01):\n",
    "    model = Sequential()\n",
    "    e = Embedding(count, 300, weights=[embedding_matrix], input_length=l, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(100,kernel_initializer='he_normal', activation='sigmoid', dropout=0.5,recurrent_dropout=0.5, unroll=False, return_sequences=False)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(lr=lrate), loss='binary_crossentropy', metrics=['acc',f1,recall,precision])\n",
    "    print('No of parameter:', model.count_params())\n",
    "    print(model.summary())\n",
    "    print(K.eval(model.optimizer.lr))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:43, 9152.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "235 words not found in vocabulary\n",
      "Preparing model...\n",
      "No of parameter: 29679901\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 926, 300)          29358900  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 29,679,901\n",
      "Trainable params: 321,001\n",
      "Non-trainable params: 29,358,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "0.01\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIRAJAHAMED\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 876 samples\n",
      "Epoch 1/3\n",
      "2880/3500 [=======================>......] - ETA: 15:58 - loss: 0.7603 - acc: 0.4688 - f1: 0.6383 - recall: 1.0000 - precision: 0.46 - ETA: 10:42 - loss: 1.0318 - acc: 0.4531 - f1: 0.3191 - recall: 0.5000 - precision: 0.23 - ETA: 8:49 - loss: 0.9363 - acc: 0.4479 - f1: 0.2128 - recall: 0.3333 - precision: 0.1562 - ETA: 7:52 - loss: 0.8469 - acc: 0.5156 - f1: 0.3687 - recall: 0.5000 - precision: 0.296 - ETA: 7:16 - loss: 0.8966 - acc: 0.5125 - f1: 0.4283 - recall: 0.6000 - precision: 0.337 - ETA: 6:52 - loss: 0.9274 - acc: 0.5052 - f1: 0.4633 - recall: 0.6667 - precision: 0.359 - ETA: 6:35 - loss: 0.9429 - acc: 0.4955 - f1: 0.4840 - recall: 0.7143 - precision: 0.370 - ETA: 6:20 - loss: 0.9297 - acc: 0.4844 - f1: 0.4933 - recall: 0.7500 - precision: 0.372 - ETA: 6:09 - loss: 0.9044 - acc: 0.5000 - f1: 0.5218 - recall: 0.7576 - precision: 0.408 - ETA: 5:59 - loss: 0.8831 - acc: 0.5031 - f1: 0.5179 - recall: 0.7256 - precision: 0.421 - ETA: 5:52 - loss: 0.8673 - acc: 0.5057 - f1: 0.5072 - recall: 0.6863 - precision: 0.439 - ETA: 5:46 - loss: 0.8585 - acc: 0.5000 - f1: 0.4733 - recall: 0.6338 - precision: 0.444 - ETA: 5:40 - loss: 0.8495 - acc: 0.4976 - f1: 0.4369 - recall: 0.5850 - precision: 0.410 - ETA: 5:33 - loss: 0.8314 - acc: 0.5134 - f1: 0.4276 - recall: 0.5562 - precision: 0.452 - ETA: 5:34 - loss: 0.8266 - acc: 0.5125 - f1: 0.4125 - recall: 0.5270 - precision: 0.466 - ETA: 5:28 - loss: 0.8188 - acc: 0.5117 - f1: 0.3867 - recall: 0.4941 - precision: 0.437 - ETA: 5:23 - loss: 0.8121 - acc: 0.5110 - f1: 0.3757 - recall: 0.4715 - precision: 0.470 - ETA: 5:18 - loss: 0.8096 - acc: 0.5052 - f1: 0.3548 - recall: 0.4453 - precision: 0.444 - ETA: 5:13 - loss: 0.8039 - acc: 0.4984 - f1: 0.3362 - recall: 0.4219 - precision: 0.421 - ETA: 5:09 - loss: 0.7953 - acc: 0.5062 - f1: 0.3432 - recall: 0.4164 - precision: 0.450 - ETA: 5:10 - loss: 0.7870 - acc: 0.5089 - f1: 0.3488 - recall: 0.4170 - precision: 0.452 - ETA: 5:12 - loss: 0.7827 - acc: 0.5057 - f1: 0.3543 - recall: 0.4182 - precision: 0.454 - ETA: 5:13 - loss: 0.7766 - acc: 0.5149 - f1: 0.3718 - recall: 0.4359 - precision: 0.465 - ETA: 5:12 - loss: 0.7722 - acc: 0.5169 - f1: 0.3847 - recall: 0.4545 - precision: 0.469 - ETA: 5:11 - loss: 0.7700 - acc: 0.5150 - f1: 0.3935 - recall: 0.4710 - precision: 0.468 - ETA: 5:11 - loss: 0.7671 - acc: 0.5168 - f1: 0.4056 - recall: 0.4913 - precision: 0.472 - ETA: 5:09 - loss: 0.7676 - acc: 0.5127 - f1: 0.4086 - recall: 0.5101 - precision: 0.466 - ETA: 5:07 - loss: 0.7663 - acc: 0.5112 - f1: 0.4142 - recall: 0.5200 - precision: 0.465 - ETA: 5:04 - loss: 0.7640 - acc: 0.5119 - f1: 0.4224 - recall: 0.5305 - precision: 0.468 - ETA: 5:02 - loss: 0.7603 - acc: 0.5115 - f1: 0.4300 - recall: 0.5422 - precision: 0.469 - ETA: 5:02 - loss: 0.7572 - acc: 0.5131 - f1: 0.4365 - recall: 0.5523 - precision: 0.470 - ETA: 5:04 - loss: 0.7534 - acc: 0.5186 - f1: 0.4449 - recall: 0.5601 - precision: 0.475 - ETA: 5:06 - loss: 0.7510 - acc: 0.5189 - f1: 0.4436 - recall: 0.5557 - precision: 0.472 - ETA: 5:06 - loss: 0.7472 - acc: 0.5257 - f1: 0.4515 - recall: 0.5590 - precision: 0.481 - ETA: 5:07 - loss: 0.7442 - acc: 0.5277 - f1: 0.4495 - recall: 0.5518 - precision: 0.482 - ETA: 5:04 - loss: 0.7430 - acc: 0.5252 - f1: 0.4456 - recall: 0.5430 - precision: 0.481 - ETA: 5:01 - loss: 0.7426 - acc: 0.5236 - f1: 0.4457 - recall: 0.5374 - precision: 0.487 - ETA: 4:57 - loss: 0.7395 - acc: 0.5280 - f1: 0.4457 - recall: 0.5320 - precision: 0.491 - ETA: 4:54 - loss: 0.7384 - acc: 0.5280 - f1: 0.4457 - recall: 0.5264 - precision: 0.498 - ETA: 4:51 - loss: 0.7383 - acc: 0.5297 - f1: 0.4465 - recall: 0.5216 - precision: 0.507 - ETA: 4:47 - loss: 0.7341 - acc: 0.5366 - f1: 0.4534 - recall: 0.5239 - precision: 0.516 - ETA: 4:44 - loss: 0.7321 - acc: 0.5365 - f1: 0.4549 - recall: 0.5220 - precision: 0.519 - ETA: 4:39 - loss: 0.7309 - acc: 0.5371 - f1: 0.4574 - recall: 0.5209 - precision: 0.523 - ETA: 4:33 - loss: 0.7292 - acc: 0.5398 - f1: 0.4621 - recall: 0.5257 - precision: 0.525 - ETA: 4:27 - loss: 0.7272 - acc: 0.5424 - f1: 0.4657 - recall: 0.5258 - precision: 0.530 - ETA: 4:21 - loss: 0.7247 - acc: 0.5462 - f1: 0.4720 - recall: 0.5296 - precision: 0.536 - ETA: 4:16 - loss: 0.7234 - acc: 0.5472 - f1: 0.4761 - recall: 0.5356 - precision: 0.537 - ETA: 4:10 - loss: 0.7223 - acc: 0.5495 - f1: 0.4801 - recall: 0.5397 - precision: 0.538 - ETA: 4:05 - loss: 0.7207 - acc: 0.5517 - f1: 0.4850 - recall: 0.5478 - precision: 0.539 - ETA: 3:59 - loss: 0.7203 - acc: 0.5531 - f1: 0.4882 - recall: 0.5568 - precision: 0.538 - ETA: 3:54 - loss: 0.7184 - acc: 0.5564 - f1: 0.4935 - recall: 0.5655 - precision: 0.539 - ETA: 3:49 - loss: 0.7184 - acc: 0.5553 - f1: 0.4955 - recall: 0.5690 - precision: 0.538 - ETA: 3:44 - loss: 0.7146 - acc: 0.5601 - f1: 0.5019 - recall: 0.5740 - precision: 0.544 - ETA: 3:39 - loss: 0.7128 - acc: 0.5631 - f1: 0.5049 - recall: 0.5773 - precision: 0.545 - ETA: 3:34 - loss: 0.7112 - acc: 0.5653 - f1: 0.5086 - recall: 0.5783 - precision: 0.550 - ETA: 3:30 - loss: 0.7101 - acc: 0.5658 - f1: 0.5103 - recall: 0.5799 - precision: 0.550 - ETA: 3:25 - loss: 0.7071 - acc: 0.5685 - f1: 0.5126 - recall: 0.5805 - precision: 0.552 - ETA: 3:20 - loss: 0.7057 - acc: 0.5706 - f1: 0.5153 - recall: 0.5806 - precision: 0.555 - ETA: 3:15 - loss: 0.7052 - acc: 0.5710 - f1: 0.5159 - recall: 0.5792 - precision: 0.557 - ETA: 3:11 - loss: 0.7032 - acc: 0.5724 - f1: 0.5176 - recall: 0.5790 - precision: 0.559 - ETA: 3:06 - loss: 0.7032 - acc: 0.5727 - f1: 0.5163 - recall: 0.5763 - precision: 0.557 - ETA: 3:02 - loss: 0.7007 - acc: 0.5741 - f1: 0.5183 - recall: 0.5794 - precision: 0.557 - ETA: 2:57 - loss: 0.7007 - acc: 0.5734 - f1: 0.5192 - recall: 0.5801 - precision: 0.557 - ETA: 2:53 - loss: 0.6996 - acc: 0.5752 - f1: 0.5211 - recall: 0.5799 - precision: 0.560 - ETA: 2:49 - loss: 0.6985 - acc: 0.5745 - f1: 0.5210 - recall: 0.5778 - precision: 0.560 - ETA: 2:44 - loss: 0.6964 - acc: 0.5762 - f1: 0.5241 - recall: 0.5806 - precision: 0.562 - ETA: 2:40 - loss: 0.6938 - acc: 0.5788 - f1: 0.5272 - recall: 0.5837 - precision: 0.564 - ETA: 2:36 - loss: 0.6932 - acc: 0.5790 - f1: 0.5284 - recall: 0.5833 - precision: 0.566 - ETA: 2:31 - loss: 0.6921 - acc: 0.5802 - f1: 0.5313 - recall: 0.5863 - precision: 0.567 - ETA: 2:27 - loss: 0.6919 - acc: 0.5804 - f1: 0.5327 - recall: 0.5877 - precision: 0.567 - ETA: 2:23 - loss: 0.6896 - acc: 0.5836 - f1: 0.5369 - recall: 0.5918 - precision: 0.571 - ETA: 2:19 - loss: 0.6883 - acc: 0.5838 - f1: 0.5382 - recall: 0.5945 - precision: 0.570 - ETA: 2:15 - loss: 0.6862 - acc: 0.5856 - f1: 0.5415 - recall: 0.5986 - precision: 0.572 - ETA: 2:11 - loss: 0.6840 - acc: 0.5874 - f1: 0.5438 - recall: 0.6004 - precision: 0.573 - ETA: 2:07 - loss: 0.6817 - acc: 0.5896 - f1: 0.5463 - recall: 0.6016 - precision: 0.576 - ETA: 2:03 - loss: 0.6799 - acc: 0.5909 - f1: 0.5484 - recall: 0.6042 - precision: 0.577 - ETA: 1:59 - loss: 0.6767 - acc: 0.5938 - f1: 0.5523 - recall: 0.6085 - precision: 0.579 - ETA: 1:55 - loss: 0.6755 - acc: 0.5950 - f1: 0.5531 - recall: 0.6110 - precision: 0.578 - ETA: 1:51 - loss: 0.6746 - acc: 0.5941 - f1: 0.5526 - recall: 0.6134 - precision: 0.576 - ETA: 1:47 - loss: 0.6739 - acc: 0.5949 - f1: 0.5540 - recall: 0.6149 - precision: 0.576 - ETA: 1:43 - loss: 0.6745 - acc: 0.5938 - f1: 0.5505 - recall: 0.6099 - precision: 0.574 - ETA: 1:39 - loss: 0.6758 - acc: 0.5941 - f1: 0.5511 - recall: 0.6086 - precision: 0.576 - ETA: 1:35 - loss: 0.6732 - acc: 0.5968 - f1: 0.5538 - recall: 0.6093 - precision: 0.580 - ETA: 1:31 - loss: 0.6717 - acc: 0.5986 - f1: 0.5564 - recall: 0.6108 - precision: 0.583 - ETA: 1:28 - loss: 0.6711 - acc: 0.5982 - f1: 0.5568 - recall: 0.6110 - precision: 0.582 - ETA: 1:24 - loss: 0.6709 - acc: 0.5981 - f1: 0.5568 - recall: 0.6123 - precision: 0.581 - ETA: 1:20 - loss: 0.6714 - acc: 0.5981 - f1: 0.5578 - recall: 0.6145 - precision: 0.580 - ETA: 1:16 - loss: 0.6722 - acc: 0.5977 - f1: 0.5584 - recall: 0.6171 - precision: 0.579 - ETA: 1:13 - loss: 0.6722 - acc: 0.5976 - f1: 0.5587 - recall: 0.6194 - precision: 0.578 - ETA: 1:09 - loss: 0.6696 - acc: 0.6003 - f1: 0.5621 - recall: 0.6236 - precision: 0.5803500/3500 [==============================] - ETA: 1:05 - loss: 0.6679 - acc: 0.6020 - f1: 0.5650 - recall: 0.6278 - precision: 0.581 - ETA: 1:02 - loss: 0.6664 - acc: 0.6029 - f1: 0.5671 - recall: 0.6288 - precision: 0.583 - ETA: 58s - loss: 0.6668 - acc: 0.6028 - f1: 0.5680 - recall: 0.6289 - precision: 0.584 - ETA: 54s - loss: 0.6657 - acc: 0.6044 - f1: 0.5702 - recall: 0.6309 - precision: 0.58 - ETA: 51s - loss: 0.6643 - acc: 0.6059 - f1: 0.5719 - recall: 0.6332 - precision: 0.58 - ETA: 47s - loss: 0.6637 - acc: 0.6064 - f1: 0.5721 - recall: 0.6349 - precision: 0.58 - ETA: 43s - loss: 0.6626 - acc: 0.6079 - f1: 0.5743 - recall: 0.6380 - precision: 0.58 - ETA: 40s - loss: 0.6612 - acc: 0.6094 - f1: 0.5762 - recall: 0.6403 - precision: 0.58 - ETA: 36s - loss: 0.6615 - acc: 0.6083 - f1: 0.5738 - recall: 0.6372 - precision: 0.58 - ETA: 33s - loss: 0.6607 - acc: 0.6078 - f1: 0.5734 - recall: 0.6362 - precision: 0.58 - ETA: 29s - loss: 0.6599 - acc: 0.6077 - f1: 0.5714 - recall: 0.6327 - precision: 0.58 - ETA: 26s - loss: 0.6592 - acc: 0.6081 - f1: 0.5700 - recall: 0.6305 - precision: 0.58 - ETA: 22s - loss: 0.6588 - acc: 0.6089 - f1: 0.5707 - recall: 0.6292 - precision: 0.58 - ETA: 19s - loss: 0.6591 - acc: 0.6088 - f1: 0.5708 - recall: 0.6275 - precision: 0.58 - ETA: 15s - loss: 0.6574 - acc: 0.6104 - f1: 0.5722 - recall: 0.6276 - precision: 0.59 - ETA: 11s - loss: 0.6563 - acc: 0.6108 - f1: 0.5727 - recall: 0.6267 - precision: 0.59 - ETA: 8s - loss: 0.6556 - acc: 0.6116 - f1: 0.5741 - recall: 0.6272 - precision: 0.5937 - ETA: 4s - loss: 0.6551 - acc: 0.6123 - f1: 0.5751 - recall: 0.6299 - precision: 0.593 - ETA: 1s - loss: 0.6540 - acc: 0.6132 - f1: 0.5770 - recall: 0.6318 - precision: 0.594 - 408s 117ms/step - loss: 0.6532 - acc: 0.6140 - f1: 0.5779 - recall: 0.6326 - precision: 0.5954 - val_loss: 0.5382 - val_acc: 0.6986 - val_f1: 0.7410 - val_recall: 0.9410 - val_precision: 0.6170\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2912/3500 [=======================>......] - ETA: 5:37 - loss: 0.4991 - acc: 0.7500 - f1: 0.7778 - recall: 0.8750 - precision: 0.700 - ETA: 5:29 - loss: 0.4974 - acc: 0.7656 - f1: 0.8111 - recall: 0.9375 - precision: 0.715 - ETA: 5:23 - loss: 0.5086 - acc: 0.7292 - f1: 0.7801 - recall: 0.9167 - precision: 0.679 - ETA: 5:19 - loss: 0.5249 - acc: 0.7109 - f1: 0.7680 - recall: 0.9081 - precision: 0.666 - ETA: 5:14 - loss: 0.5447 - acc: 0.6937 - f1: 0.7512 - recall: 0.8998 - precision: 0.645 - ETA: 5:12 - loss: 0.5461 - acc: 0.6823 - f1: 0.7371 - recall: 0.8832 - precision: 0.633 - ETA: 5:08 - loss: 0.5290 - acc: 0.6964 - f1: 0.7490 - recall: 0.8999 - precision: 0.642 - ETA: 5:04 - loss: 0.5141 - acc: 0.7148 - f1: 0.7589 - recall: 0.8756 - precision: 0.687 - ETA: 5:01 - loss: 0.5409 - acc: 0.6944 - f1: 0.7190 - recall: 0.8154 - precision: 0.666 - ETA: 4:58 - loss: 0.5593 - acc: 0.6813 - f1: 0.6932 - recall: 0.7713 - precision: 0.659 - ETA: 4:55 - loss: 0.5651 - acc: 0.6761 - f1: 0.6870 - recall: 0.7491 - precision: 0.669 - ETA: 4:52 - loss: 0.5665 - acc: 0.6745 - f1: 0.6835 - recall: 0.7462 - precision: 0.662 - ETA: 4:49 - loss: 0.5695 - acc: 0.6755 - f1: 0.6853 - recall: 0.7400 - precision: 0.669 - ETA: 4:46 - loss: 0.5602 - acc: 0.6830 - f1: 0.6814 - recall: 0.7300 - precision: 0.669 - ETA: 4:44 - loss: 0.5589 - acc: 0.6833 - f1: 0.6831 - recall: 0.7314 - precision: 0.669 - ETA: 4:41 - loss: 0.5553 - acc: 0.6895 - f1: 0.6904 - recall: 0.7343 - precision: 0.678 - ETA: 4:38 - loss: 0.5539 - acc: 0.6930 - f1: 0.6968 - recall: 0.7464 - precision: 0.679 - ETA: 4:35 - loss: 0.5527 - acc: 0.6962 - f1: 0.7006 - recall: 0.7501 - precision: 0.682 - ETA: 4:31 - loss: 0.5657 - acc: 0.6908 - f1: 0.6968 - recall: 0.7633 - precision: 0.670 - ETA: 4:29 - loss: 0.5655 - acc: 0.6922 - f1: 0.6998 - recall: 0.7688 - precision: 0.670 - ETA: 4:26 - loss: 0.5652 - acc: 0.6949 - f1: 0.7029 - recall: 0.7765 - precision: 0.669 - ETA: 4:23 - loss: 0.5646 - acc: 0.6960 - f1: 0.7053 - recall: 0.7836 - precision: 0.667 - ETA: 4:21 - loss: 0.5646 - acc: 0.6957 - f1: 0.7000 - recall: 0.7772 - precision: 0.662 - ETA: 4:18 - loss: 0.5628 - acc: 0.7005 - f1: 0.7056 - recall: 0.7795 - precision: 0.669 - ETA: 4:17 - loss: 0.5618 - acc: 0.7013 - f1: 0.7030 - recall: 0.7712 - precision: 0.671 - ETA: 4:15 - loss: 0.5581 - acc: 0.7067 - f1: 0.7073 - recall: 0.7680 - precision: 0.684 - ETA: 4:12 - loss: 0.5573 - acc: 0.7037 - f1: 0.7010 - recall: 0.7595 - precision: 0.678 - ETA: 4:09 - loss: 0.5581 - acc: 0.7020 - f1: 0.6971 - recall: 0.7492 - precision: 0.683 - ETA: 4:06 - loss: 0.5607 - acc: 0.7015 - f1: 0.6990 - recall: 0.7449 - precision: 0.691 - ETA: 4:02 - loss: 0.5567 - acc: 0.7031 - f1: 0.6969 - recall: 0.7380 - precision: 0.694 - ETA: 4:00 - loss: 0.5558 - acc: 0.7036 - f1: 0.6979 - recall: 0.7400 - precision: 0.693 - ETA: 3:56 - loss: 0.5573 - acc: 0.7051 - f1: 0.6995 - recall: 0.7389 - precision: 0.697 - ETA: 3:53 - loss: 0.5543 - acc: 0.7074 - f1: 0.7032 - recall: 0.7435 - precision: 0.699 - ETA: 3:51 - loss: 0.5509 - acc: 0.7105 - f1: 0.7051 - recall: 0.7442 - precision: 0.701 - ETA: 3:48 - loss: 0.5485 - acc: 0.7125 - f1: 0.7086 - recall: 0.7500 - precision: 0.702 - ETA: 3:45 - loss: 0.5495 - acc: 0.7127 - f1: 0.7109 - recall: 0.7506 - precision: 0.705 - ETA: 3:42 - loss: 0.5495 - acc: 0.7103 - f1: 0.7102 - recall: 0.7537 - precision: 0.701 - ETA: 3:39 - loss: 0.5439 - acc: 0.7146 - f1: 0.7152 - recall: 0.7602 - precision: 0.704 - ETA: 3:36 - loss: 0.5443 - acc: 0.7139 - f1: 0.7158 - recall: 0.7618 - precision: 0.703 - ETA: 3:33 - loss: 0.5464 - acc: 0.7156 - f1: 0.7186 - recall: 0.7678 - precision: 0.703 - ETA: 3:30 - loss: 0.5454 - acc: 0.7165 - f1: 0.7189 - recall: 0.7682 - precision: 0.703 - ETA: 3:27 - loss: 0.5480 - acc: 0.7173 - f1: 0.7206 - recall: 0.7737 - precision: 0.702 - ETA: 3:24 - loss: 0.5493 - acc: 0.7151 - f1: 0.7189 - recall: 0.7754 - precision: 0.697 - ETA: 3:21 - loss: 0.5481 - acc: 0.7166 - f1: 0.7202 - recall: 0.7760 - precision: 0.699 - ETA: 3:18 - loss: 0.5467 - acc: 0.7167 - f1: 0.7177 - recall: 0.7717 - precision: 0.697 - ETA: 3:15 - loss: 0.5435 - acc: 0.7194 - f1: 0.7181 - recall: 0.7718 - precision: 0.697 - ETA: 3:11 - loss: 0.5405 - acc: 0.7207 - f1: 0.7176 - recall: 0.7668 - precision: 0.704 - ETA: 3:08 - loss: 0.5439 - acc: 0.7181 - f1: 0.7117 - recall: 0.7569 - precision: 0.706 - ETA: 3:05 - loss: 0.5422 - acc: 0.7207 - f1: 0.7138 - recall: 0.7555 - precision: 0.712 - ETA: 3:03 - loss: 0.5438 - acc: 0.7175 - f1: 0.7040 - recall: 0.7435 - precision: 0.706 - ETA: 2:59 - loss: 0.5412 - acc: 0.7188 - f1: 0.7026 - recall: 0.7379 - precision: 0.712 - ETA: 2:57 - loss: 0.5419 - acc: 0.7163 - f1: 0.6974 - recall: 0.7311 - precision: 0.708 - ETA: 2:54 - loss: 0.5410 - acc: 0.7164 - f1: 0.6973 - recall: 0.7299 - precision: 0.708 - ETA: 2:51 - loss: 0.5397 - acc: 0.7176 - f1: 0.6994 - recall: 0.7310 - precision: 0.710 - ETA: 2:48 - loss: 0.5387 - acc: 0.7182 - f1: 0.7012 - recall: 0.7323 - precision: 0.712 - ETA: 2:45 - loss: 0.5367 - acc: 0.7193 - f1: 0.7032 - recall: 0.7341 - precision: 0.713 - ETA: 2:41 - loss: 0.5359 - acc: 0.7193 - f1: 0.7045 - recall: 0.7388 - precision: 0.712 - ETA: 2:38 - loss: 0.5352 - acc: 0.7198 - f1: 0.7060 - recall: 0.7412 - precision: 0.712 - ETA: 2:35 - loss: 0.5345 - acc: 0.7203 - f1: 0.7065 - recall: 0.7442 - precision: 0.710 - ETA: 2:32 - loss: 0.5369 - acc: 0.7188 - f1: 0.7064 - recall: 0.7485 - precision: 0.707 - ETA: 2:29 - loss: 0.5393 - acc: 0.7162 - f1: 0.7054 - recall: 0.7487 - precision: 0.705 - ETA: 2:26 - loss: 0.5406 - acc: 0.7152 - f1: 0.7059 - recall: 0.7518 - precision: 0.703 - ETA: 2:23 - loss: 0.5383 - acc: 0.7178 - f1: 0.7091 - recall: 0.7550 - precision: 0.706 - ETA: 2:20 - loss: 0.5368 - acc: 0.7188 - f1: 0.7103 - recall: 0.7559 - precision: 0.707 - ETA: 2:17 - loss: 0.5347 - acc: 0.7202 - f1: 0.7119 - recall: 0.7585 - precision: 0.707 - ETA: 2:13 - loss: 0.5350 - acc: 0.7211 - f1: 0.7138 - recall: 0.7594 - precision: 0.709 - ETA: 2:10 - loss: 0.5325 - acc: 0.7234 - f1: 0.7162 - recall: 0.7611 - precision: 0.712 - ETA: 2:07 - loss: 0.5319 - acc: 0.7233 - f1: 0.7168 - recall: 0.7602 - precision: 0.714 - ETA: 2:04 - loss: 0.5313 - acc: 0.7237 - f1: 0.7170 - recall: 0.7586 - precision: 0.715 - ETA: 2:01 - loss: 0.5286 - acc: 0.7268 - f1: 0.7201 - recall: 0.7601 - precision: 0.719 - ETA: 1:58 - loss: 0.5295 - acc: 0.7262 - f1: 0.7193 - recall: 0.7588 - precision: 0.719 - ETA: 1:55 - loss: 0.5296 - acc: 0.7270 - f1: 0.7201 - recall: 0.7587 - precision: 0.720 - ETA: 1:52 - loss: 0.5287 - acc: 0.7277 - f1: 0.7208 - recall: 0.7593 - precision: 0.720 - ETA: 1:49 - loss: 0.5273 - acc: 0.7289 - f1: 0.7220 - recall: 0.7607 - precision: 0.721 - ETA: 1:46 - loss: 0.5239 - acc: 0.7312 - f1: 0.7247 - recall: 0.7632 - precision: 0.723 - ETA: 1:43 - loss: 0.5218 - acc: 0.7327 - f1: 0.7264 - recall: 0.7641 - precision: 0.725 - ETA: 1:40 - loss: 0.5231 - acc: 0.7321 - f1: 0.7259 - recall: 0.7631 - precision: 0.725 - ETA: 1:37 - loss: 0.5217 - acc: 0.7328 - f1: 0.7255 - recall: 0.7647 - precision: 0.723 - ETA: 1:34 - loss: 0.5220 - acc: 0.7326 - f1: 0.7255 - recall: 0.7640 - precision: 0.723 - ETA: 1:32 - loss: 0.5219 - acc: 0.7328 - f1: 0.7256 - recall: 0.7650 - precision: 0.722 - ETA: 1:29 - loss: 0.5199 - acc: 0.7338 - f1: 0.7267 - recall: 0.7663 - precision: 0.723 - ETA: 1:26 - loss: 0.5178 - acc: 0.7351 - f1: 0.7282 - recall: 0.7676 - precision: 0.724 - ETA: 1:22 - loss: 0.5188 - acc: 0.7334 - f1: 0.7257 - recall: 0.7648 - precision: 0.721 - ETA: 1:19 - loss: 0.5179 - acc: 0.7344 - f1: 0.7268 - recall: 0.7645 - precision: 0.724 - ETA: 1:16 - loss: 0.5177 - acc: 0.7346 - f1: 0.7267 - recall: 0.7628 - precision: 0.725 - ETA: 1:13 - loss: 0.5173 - acc: 0.7347 - f1: 0.7263 - recall: 0.7627 - precision: 0.724 - ETA: 1:10 - loss: 0.5170 - acc: 0.7342 - f1: 0.7256 - recall: 0.7611 - precision: 0.724 - ETA: 1:07 - loss: 0.5159 - acc: 0.7351 - f1: 0.7263 - recall: 0.7621 - precision: 0.724 - ETA: 1:04 - loss: 0.5155 - acc: 0.7349 - f1: 0.7256 - recall: 0.7619 - precision: 0.723 - ETA: 1:00 - loss: 0.5125 - acc: 0.7368 - f1: 0.7277 - recall: 0.7627 - precision: 0.726 - ETA: 57s - loss: 0.5132 - acc: 0.7366 - f1: 0.7280 - recall: 0.7620 - precision: 0.7272 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500/3500 [==============================] - ETA: 54s - loss: 0.5121 - acc: 0.7368 - f1: 0.7276 - recall: 0.7619 - precision: 0.72 - ETA: 51s - loss: 0.5105 - acc: 0.7379 - f1: 0.7290 - recall: 0.7632 - precision: 0.72 - ETA: 48s - loss: 0.5101 - acc: 0.7380 - f1: 0.7295 - recall: 0.7639 - precision: 0.72 - ETA: 45s - loss: 0.5095 - acc: 0.7391 - f1: 0.7310 - recall: 0.7658 - precision: 0.72 - ETA: 42s - loss: 0.5080 - acc: 0.7409 - f1: 0.7328 - recall: 0.7669 - precision: 0.73 - ETA: 38s - loss: 0.5092 - acc: 0.7407 - f1: 0.7329 - recall: 0.7679 - precision: 0.72 - ETA: 35s - loss: 0.5088 - acc: 0.7411 - f1: 0.7338 - recall: 0.7692 - precision: 0.73 - ETA: 32s - loss: 0.5088 - acc: 0.7412 - f1: 0.7341 - recall: 0.7696 - precision: 0.73 - ETA: 29s - loss: 0.5091 - acc: 0.7412 - f1: 0.7344 - recall: 0.7719 - precision: 0.72 - ETA: 26s - loss: 0.5094 - acc: 0.7416 - f1: 0.7346 - recall: 0.7726 - precision: 0.72 - ETA: 23s - loss: 0.5085 - acc: 0.7420 - f1: 0.7345 - recall: 0.7714 - precision: 0.72 - ETA: 20s - loss: 0.5083 - acc: 0.7427 - f1: 0.7350 - recall: 0.7710 - precision: 0.73 - ETA: 16s - loss: 0.5086 - acc: 0.7419 - f1: 0.7336 - recall: 0.7691 - precision: 0.72 - ETA: 13s - loss: 0.5084 - acc: 0.7423 - f1: 0.7337 - recall: 0.7681 - precision: 0.73 - ETA: 10s - loss: 0.5065 - acc: 0.7435 - f1: 0.7351 - recall: 0.7697 - precision: 0.73 - ETA: 7s - loss: 0.5092 - acc: 0.7421 - f1: 0.7343 - recall: 0.7684 - precision: 0.7307 - ETA: 4s - loss: 0.5100 - acc: 0.7419 - f1: 0.7342 - recall: 0.7675 - precision: 0.731 - ETA: 1s - loss: 0.5095 - acc: 0.7423 - f1: 0.7346 - recall: 0.7689 - precision: 0.730 - 364s 104ms/step - loss: 0.5104 - acc: 0.7417 - f1: 0.7342 - recall: 0.7682 - precision: 0.7306 - val_loss: 0.4260 - val_acc: 0.7968 - val_f1: 0.8013 - val_recall: 0.8984 - val_precision: 0.7307\n",
      "Epoch 3/3\n",
      "2912/3500 [=======================>......] - ETA: 5:46 - loss: 0.4063 - acc: 0.8125 - f1: 0.8421 - recall: 0.8889 - precision: 0.800 - ETA: 5:45 - loss: 0.3848 - acc: 0.8438 - f1: 0.8655 - recall: 0.9150 - precision: 0.821 - ETA: 5:40 - loss: 0.3733 - acc: 0.8438 - f1: 0.8733 - recall: 0.9275 - precision: 0.825 - ETA: 6:11 - loss: 0.3942 - acc: 0.8438 - f1: 0.8619 - recall: 0.8956 - precision: 0.833 - ETA: 6:10 - loss: 0.3784 - acc: 0.8500 - f1: 0.8645 - recall: 0.9032 - precision: 0.831 - ETA: 5:57 - loss: 0.3747 - acc: 0.8594 - f1: 0.8755 - recall: 0.9041 - precision: 0.851 - ETA: 5:49 - loss: 0.3910 - acc: 0.8571 - f1: 0.8728 - recall: 0.9089 - precision: 0.842 - ETA: 5:41 - loss: 0.4031 - acc: 0.8398 - f1: 0.8546 - recall: 0.8953 - precision: 0.820 - ETA: 5:35 - loss: 0.4003 - acc: 0.8368 - f1: 0.8512 - recall: 0.8930 - precision: 0.815 - ETA: 5:30 - loss: 0.4060 - acc: 0.8313 - f1: 0.8471 - recall: 0.8871 - precision: 0.813 - ETA: 5:25 - loss: 0.4120 - acc: 0.8210 - f1: 0.8283 - recall: 0.8872 - precision: 0.784 - ETA: 5:21 - loss: 0.4189 - acc: 0.8151 - f1: 0.8260 - recall: 0.8966 - precision: 0.774 - ETA: 5:17 - loss: 0.4212 - acc: 0.8101 - f1: 0.8174 - recall: 0.8868 - precision: 0.766 - ETA: 5:12 - loss: 0.4304 - acc: 0.8013 - f1: 0.8081 - recall: 0.8796 - precision: 0.755 - ETA: 5:08 - loss: 0.4248 - acc: 0.8063 - f1: 0.8106 - recall: 0.8774 - precision: 0.761 - ETA: 5:05 - loss: 0.4371 - acc: 0.7988 - f1: 0.8016 - recall: 0.8672 - precision: 0.753 - ETA: 5:01 - loss: 0.4372 - acc: 0.7941 - f1: 0.7921 - recall: 0.8554 - precision: 0.744 - ETA: 4:56 - loss: 0.4394 - acc: 0.7917 - f1: 0.7889 - recall: 0.8418 - precision: 0.754 - ETA: 4:52 - loss: 0.4412 - acc: 0.7928 - f1: 0.7856 - recall: 0.8326 - precision: 0.756 - ETA: 4:48 - loss: 0.4551 - acc: 0.7844 - f1: 0.7713 - recall: 0.8097 - precision: 0.756 - ETA: 4:44 - loss: 0.4521 - acc: 0.7887 - f1: 0.7754 - recall: 0.8069 - precision: 0.768 - ETA: 4:41 - loss: 0.4561 - acc: 0.7884 - f1: 0.7775 - recall: 0.8033 - precision: 0.775 - ETA: 4:37 - loss: 0.4520 - acc: 0.7921 - f1: 0.7805 - recall: 0.8025 - precision: 0.782 - ETA: 4:34 - loss: 0.4509 - acc: 0.7943 - f1: 0.7840 - recall: 0.8061 - precision: 0.784 - ETA: 4:30 - loss: 0.4489 - acc: 0.7950 - f1: 0.7817 - recall: 0.8094 - precision: 0.777 - ETA: 4:27 - loss: 0.4489 - acc: 0.7957 - f1: 0.7829 - recall: 0.8116 - precision: 0.777 - ETA: 4:23 - loss: 0.4486 - acc: 0.7940 - f1: 0.7822 - recall: 0.8099 - precision: 0.776 - ETA: 4:20 - loss: 0.4462 - acc: 0.7935 - f1: 0.7832 - recall: 0.8107 - precision: 0.777 - ETA: 4:17 - loss: 0.4419 - acc: 0.7942 - f1: 0.7853 - recall: 0.8152 - precision: 0.776 - ETA: 4:13 - loss: 0.4481 - acc: 0.7917 - f1: 0.7839 - recall: 0.8190 - precision: 0.771 - ETA: 4:10 - loss: 0.4448 - acc: 0.7933 - f1: 0.7871 - recall: 0.8232 - precision: 0.773 - ETA: 4:07 - loss: 0.4473 - acc: 0.7891 - f1: 0.7839 - recall: 0.8287 - precision: 0.765 - ETA: 4:03 - loss: 0.4471 - acc: 0.7888 - f1: 0.7844 - recall: 0.8339 - precision: 0.762 - ETA: 4:00 - loss: 0.4452 - acc: 0.7895 - f1: 0.7849 - recall: 0.8329 - precision: 0.763 - ETA: 3:57 - loss: 0.4470 - acc: 0.7884 - f1: 0.7815 - recall: 0.8282 - precision: 0.760 - ETA: 3:54 - loss: 0.4472 - acc: 0.7908 - f1: 0.7846 - recall: 0.8277 - precision: 0.767 - ETA: 3:51 - loss: 0.4452 - acc: 0.7905 - f1: 0.7839 - recall: 0.8218 - precision: 0.773 - ETA: 3:48 - loss: 0.4485 - acc: 0.7887 - f1: 0.7757 - recall: 0.8090 - precision: 0.774 - ETA: 3:45 - loss: 0.4503 - acc: 0.7861 - f1: 0.7734 - recall: 0.8016 - precision: 0.780 - ETA: 3:41 - loss: 0.4515 - acc: 0.7852 - f1: 0.7728 - recall: 0.7983 - precision: 0.782 - ETA: 3:38 - loss: 0.4529 - acc: 0.7843 - f1: 0.7726 - recall: 0.7955 - precision: 0.784 - ETA: 3:35 - loss: 0.4480 - acc: 0.7879 - f1: 0.7763 - recall: 0.7987 - precision: 0.787 - ETA: 3:32 - loss: 0.4495 - acc: 0.7878 - f1: 0.7745 - recall: 0.7970 - precision: 0.784 - ETA: 3:29 - loss: 0.4548 - acc: 0.7848 - f1: 0.7715 - recall: 0.7964 - precision: 0.779 - ETA: 3:26 - loss: 0.4533 - acc: 0.7854 - f1: 0.7734 - recall: 0.7968 - precision: 0.782 - ETA: 3:23 - loss: 0.4512 - acc: 0.7867 - f1: 0.7746 - recall: 0.7982 - precision: 0.782 - ETA: 3:20 - loss: 0.4513 - acc: 0.7859 - f1: 0.7737 - recall: 0.7968 - precision: 0.781 - ETA: 3:17 - loss: 0.4527 - acc: 0.7852 - f1: 0.7735 - recall: 0.7982 - precision: 0.779 - ETA: 3:14 - loss: 0.4551 - acc: 0.7844 - f1: 0.7719 - recall: 0.8003 - precision: 0.775 - ETA: 3:10 - loss: 0.4578 - acc: 0.7844 - f1: 0.7716 - recall: 0.8012 - precision: 0.773 - ETA: 3:07 - loss: 0.4555 - acc: 0.7862 - f1: 0.7731 - recall: 0.8021 - precision: 0.774 - ETA: 3:04 - loss: 0.4551 - acc: 0.7861 - f1: 0.7728 - recall: 0.8043 - precision: 0.772 - ETA: 3:01 - loss: 0.4535 - acc: 0.7883 - f1: 0.7751 - recall: 0.8080 - precision: 0.773 - ETA: 2:57 - loss: 0.4557 - acc: 0.7870 - f1: 0.7752 - recall: 0.8065 - precision: 0.774 - ETA: 2:54 - loss: 0.4562 - acc: 0.7858 - f1: 0.7728 - recall: 0.8022 - precision: 0.773 - ETA: 2:51 - loss: 0.4532 - acc: 0.7868 - f1: 0.7730 - recall: 0.7994 - precision: 0.777 - ETA: 2:48 - loss: 0.4521 - acc: 0.7884 - f1: 0.7751 - recall: 0.8011 - precision: 0.779 - ETA: 2:45 - loss: 0.4512 - acc: 0.7893 - f1: 0.7760 - recall: 0.8020 - precision: 0.780 - ETA: 2:42 - loss: 0.4502 - acc: 0.7908 - f1: 0.7775 - recall: 0.8022 - precision: 0.782 - ETA: 2:39 - loss: 0.4486 - acc: 0.7917 - f1: 0.7782 - recall: 0.8029 - precision: 0.782 - ETA: 2:37 - loss: 0.4473 - acc: 0.7925 - f1: 0.7791 - recall: 0.8023 - precision: 0.784 - ETA: 2:36 - loss: 0.4460 - acc: 0.7939 - f1: 0.7806 - recall: 0.8034 - precision: 0.786 - ETA: 2:33 - loss: 0.4466 - acc: 0.7946 - f1: 0.7816 - recall: 0.8037 - precision: 0.787 - ETA: 2:30 - loss: 0.4460 - acc: 0.7954 - f1: 0.7830 - recall: 0.8059 - precision: 0.788 - ETA: 2:26 - loss: 0.4465 - acc: 0.7942 - f1: 0.7816 - recall: 0.8045 - precision: 0.786 - ETA: 2:23 - loss: 0.4453 - acc: 0.7950 - f1: 0.7829 - recall: 0.8065 - precision: 0.786 - ETA: 2:20 - loss: 0.4438 - acc: 0.7957 - f1: 0.7829 - recall: 0.8079 - precision: 0.784 - ETA: 2:16 - loss: 0.4430 - acc: 0.7964 - f1: 0.7842 - recall: 0.8085 - precision: 0.786 - ETA: 2:13 - loss: 0.4408 - acc: 0.7976 - f1: 0.7859 - recall: 0.8099 - precision: 0.788 - ETA: 2:10 - loss: 0.4410 - acc: 0.7969 - f1: 0.7857 - recall: 0.8101 - precision: 0.787 - ETA: 2:06 - loss: 0.4387 - acc: 0.7975 - f1: 0.7870 - recall: 0.8120 - precision: 0.787 - ETA: 2:03 - loss: 0.4356 - acc: 0.7995 - f1: 0.7894 - recall: 0.8140 - precision: 0.790 - ETA: 2:00 - loss: 0.4366 - acc: 0.7997 - f1: 0.7898 - recall: 0.8156 - precision: 0.789 - ETA: 1:56 - loss: 0.4344 - acc: 0.8007 - f1: 0.7913 - recall: 0.8168 - precision: 0.791 - ETA: 1:53 - loss: 0.4377 - acc: 0.7983 - f1: 0.7888 - recall: 0.8168 - precision: 0.786 - ETA: 1:50 - loss: 0.4387 - acc: 0.7977 - f1: 0.7883 - recall: 0.8173 - precision: 0.785 - ETA: 1:46 - loss: 0.4386 - acc: 0.7975 - f1: 0.7887 - recall: 0.8171 - precision: 0.785 - ETA: 1:43 - loss: 0.4377 - acc: 0.7985 - f1: 0.7903 - recall: 0.8188 - precision: 0.787 - ETA: 1:40 - loss: 0.4362 - acc: 0.7991 - f1: 0.7910 - recall: 0.8196 - precision: 0.787 - ETA: 1:36 - loss: 0.4375 - acc: 0.7977 - f1: 0.7905 - recall: 0.8175 - precision: 0.788 - ETA: 1:33 - loss: 0.4374 - acc: 0.7975 - f1: 0.7903 - recall: 0.8172 - precision: 0.788 - ETA: 1:30 - loss: 0.4407 - acc: 0.7957 - f1: 0.7885 - recall: 0.8145 - precision: 0.787 - ETA: 1:26 - loss: 0.4416 - acc: 0.7952 - f1: 0.7884 - recall: 0.8131 - precision: 0.788 - ETA: 1:23 - loss: 0.4406 - acc: 0.7954 - f1: 0.7887 - recall: 0.8131 - precision: 0.788 - ETA: 1:20 - loss: 0.4404 - acc: 0.7963 - f1: 0.7900 - recall: 0.8141 - precision: 0.790 - ETA: 1:16 - loss: 0.4398 - acc: 0.7965 - f1: 0.7904 - recall: 0.8155 - precision: 0.789 - ETA: 1:13 - loss: 0.4391 - acc: 0.7971 - f1: 0.7909 - recall: 0.8176 - precision: 0.788 - ETA: 1:10 - loss: 0.4392 - acc: 0.7962 - f1: 0.7902 - recall: 0.8180 - precision: 0.786 - ETA: 1:06 - loss: 0.4384 - acc: 0.7974 - f1: 0.7917 - recall: 0.8190 - precision: 0.788 - ETA: 1:03 - loss: 0.4383 - acc: 0.7972 - f1: 0.7915 - recall: 0.8201 - precision: 0.787 - ETA: 1:00 - loss: 0.4390 - acc: 0.7967 - f1: 0.7907 - recall: 0.8184 - precision: 0.78703500/3500 [==============================] - ETA: 57s - loss: 0.4383 - acc: 0.7976 - f1: 0.7916 - recall: 0.8204 - precision: 0.786 - ETA: 53s - loss: 0.4370 - acc: 0.7984 - f1: 0.7924 - recall: 0.8216 - precision: 0.78 - ETA: 50s - loss: 0.4380 - acc: 0.7975 - f1: 0.7919 - recall: 0.8201 - precision: 0.78 - ETA: 47s - loss: 0.4375 - acc: 0.7977 - f1: 0.7912 - recall: 0.8185 - precision: 0.78 - ETA: 43s - loss: 0.4368 - acc: 0.7975 - f1: 0.7907 - recall: 0.8169 - precision: 0.78 - ETA: 40s - loss: 0.4362 - acc: 0.7970 - f1: 0.7887 - recall: 0.8173 - precision: 0.78 - ETA: 37s - loss: 0.4347 - acc: 0.7975 - f1: 0.7891 - recall: 0.8177 - precision: 0.78 - ETA: 34s - loss: 0.4334 - acc: 0.7986 - f1: 0.7902 - recall: 0.8189 - precision: 0.78 - ETA: 30s - loss: 0.4332 - acc: 0.7978 - f1: 0.7896 - recall: 0.8178 - precision: 0.78 - ETA: 27s - loss: 0.4331 - acc: 0.7973 - f1: 0.7895 - recall: 0.8163 - precision: 0.78 - ETA: 24s - loss: 0.4328 - acc: 0.7969 - f1: 0.7885 - recall: 0.8163 - precision: 0.78 - ETA: 20s - loss: 0.4334 - acc: 0.7967 - f1: 0.7879 - recall: 0.8163 - precision: 0.78 - ETA: 17s - loss: 0.4318 - acc: 0.7978 - f1: 0.7888 - recall: 0.8173 - precision: 0.78 - ETA: 14s - loss: 0.4311 - acc: 0.7988 - f1: 0.7899 - recall: 0.8178 - precision: 0.78 - ETA: 11s - loss: 0.4324 - acc: 0.7981 - f1: 0.7889 - recall: 0.8160 - precision: 0.78 - ETA: 7s - loss: 0.4316 - acc: 0.7982 - f1: 0.7890 - recall: 0.8170 - precision: 0.7849 - ETA: 4s - loss: 0.4304 - acc: 0.7992 - f1: 0.7903 - recall: 0.8182 - precision: 0.785 - ETA: 1s - loss: 0.4312 - acc: 0.7987 - f1: 0.7899 - recall: 0.8176 - precision: 0.785 - 386s 110ms/step - loss: 0.4310 - acc: 0.7989 - f1: 0.7899 - recall: 0.8182 - precision: 0.7852 - val_loss: 0.3298 - val_acc: 0.8596 - val_f1: 0.8512 - val_recall: 0.9037 - val_precision: 0.8102\n",
      "1095/1095 [==============================] - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 24 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 26s 23ms/step\n",
      "Accuracy: 86.666667\n",
      "F1 Score: 86.165775\n",
      "Recall: 89.408976\n",
      "Precision: 83.918042\n"
     ]
    }
   ],
   "source": [
    "def ReadTest(filename,Labelfile):\n",
    "    data = []\n",
    "\n",
    "    with codecs.open(filename, 'r',encoding=\"utf-8\", errors=\"replace\") as readFile:\n",
    "        reader = csv.reader(readFile)\n",
    "        lines = list(reader)\n",
    "    count = 0\n",
    "    for i in lines:\n",
    "        temp = []\n",
    "        sentence = ' '.join(i)\n",
    "        for j in word_tokenize(sentence):\n",
    "            temp.append(j.lower()) \n",
    "            count += 1\n",
    "\n",
    "        data.append(temp)\n",
    "\n",
    "    labels_pd = pd.read_csv(Labelfile,index_col=False)\n",
    "    labels = numpy.array(labels_pd['Labels'])\n",
    "    # labels = numpy.asarray(labels_pd)\n",
    "\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(data)\n",
    "    encoded_docs = t.texts_to_sequences(data)\n",
    "    padded_docs = pad_sequences(encoded_docs, padding='post',maxlen=926)\n",
    "\n",
    "    return padded_docs,labels\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open('settings.json') as data_file:\n",
    "        data = json.load(data_file)\n",
    "\n",
    "    lrate = data[\"Model_settings\"][\"Learning_rate\"]\n",
    "    num_epochs = data[\"Model_settings\"][\"Epochs\"]\n",
    "\n",
    "    filename = data[\"FileNames\"][\"Training_file\"]\n",
    "    Labelfile = data[\"FileNames\"][\"Label_file\"]\n",
    "    # data,labels = ReadFile(filename,Labelfile)\n",
    "\n",
    "\n",
    "    print('Reading data...')\n",
    "    data,labels,count = ReadOpen(filename,Labelfile)\n",
    "    print('Getting Embeddings...')\n",
    "    padded_docs, embedding_matrix,l = Preprocess(data,count)\n",
    "    print('Preparing model...')\n",
    "    model = PrepModel(count,embedding_matrix,l,lrate)\n",
    "    print('Training...')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n",
    "    # X_test1, y_test1 = ReadTest(\"Testing_data.csv\",\"Testing_labels.csv\")\n",
    "    # print(\"length of X_test,y_test\")\n",
    "    # print(len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=1, mode='auto')\n",
    "    model.fit(X_train, y_train, validation_data=(X_val,y_val), nb_epoch=3, verbose=1, callbacks=[earlyStopping])\n",
    "    loss, accuracy,f1_score,recall_score,precision_score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    \n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    print('F1 Score: %f' % (f1_score*100))\n",
    "    print('Recall: %f' % (recall_score*100))\n",
    "    print('Precision: %f' % (precision_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
